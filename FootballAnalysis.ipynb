{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "W5mSi1POvrzh",
        "3ukV54AYv080"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3XA_2l3zTac",
        "outputId": "ca22fe4a-b0fb-426e-8441-263653833d5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/13.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/13.6 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/13.6 MB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m260.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m139.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip -q install opencv-python-headless scikit-learn matplotlib tqdm pandas\n",
        "# optional but faster video decode\n",
        "!pip -q install decord"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create files"
      ],
      "metadata": {
        "id": "J9-1YKru1hna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ],
      "metadata": {
        "id": "_HTBuELYD0wm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, textwrap, pathlib\n",
        "\n",
        "ROOT = pathlib.Path(\"/content/soccer_rnn\")\n",
        "ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def write(relpath: str, content: str):\n",
        "    p = ROOT / relpath\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    p.write_text(textwrap.dedent(content).lstrip(), encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "Z-Mh5Wnw0Nhb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile soccer_rnn/__init__.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTbhgLVh0QoM",
        "outputId": "0b55a40f-6c09-4262-aa9d-2d8a24ba3cdd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing soccer_rnn/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile soccer_rnn/config.py\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # data\n",
        "    dataset_source: str = \"kaggle\"  # \"kaggle\" or \"soccernet\"\n",
        "    raw_root: str = \"/content/data_raw\"\n",
        "    processed_root: str = \"/content/data_processed\"\n",
        "    cache_root: str = \"/content/feature_cache\"\n",
        "\n",
        "    # for clip-based classification\n",
        "    clip_num_frames: int = 32\n",
        "    clip_fps: int = 8\n",
        "    clip_radius_sec: float = 1.5  # only for SoccerNet extraction (window = 2*radius)\n",
        "    image_size: int = 224\n",
        "\n",
        "    # split\n",
        "    seed: int = 42\n",
        "    test_size: float = 0.15\n",
        "    val_size: float = 0.15  # from remaining\n",
        "\n",
        "    # train\n",
        "    batch_size: int = 16\n",
        "    num_workers: int = 0\n",
        "    epochs: int = 10\n",
        "    lr: float = 1e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    grad_clip: float = 1.0\n",
        "    use_amp: bool = True\n",
        "\n",
        "    # model\n",
        "    cnn_backbone: str = \"resnet18\"  # fixed in this baseline\n",
        "    feature_dim: int = 512\n",
        "    hidden_size: int = 256\n",
        "    num_layers: int = 1\n",
        "    dropout: float = 0.2\n",
        "\n",
        "    # output\n",
        "    out_root: str = \"/content/runs\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WpHWzih0ODy",
        "outputId": "1c6b63b5-7c26-4fc2-b97b-34302aede388"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting soccer_rnn/config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir(\"soccer_rnn/data\")\n",
        "os.mkdir(\"soccer_rnn/models\")\n",
        "os.mkdir(\"soccer_rnn/utils\")"
      ],
      "metadata": {
        "id": "30rP5Hdq1JFN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile soccer_rnn/utils/seed.py\n",
        "import random, os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcU6dAlz0Yba",
        "outputId": "34277dd1-bec8-4713-c694-8c0c5ac25f91"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing soccer_rnn/utils/seed.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile soccer_rnn/data/indexing.py\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "VIDEO_EXTS = {\".mp4\", \".mkv\", \".avi\", \".mov\", \".webm\"}\n",
        "\n",
        "def build_index_folder_per_class(dataset_dir: str, out_csv: str):\n",
        "    '''\n",
        "    Expects:\n",
        "      dataset_dir/\n",
        "        Pass/*.mp4\n",
        "        Shot/*.mp4\n",
        "        Dribble/*.mp4\n",
        "      or any set of class folders.\n",
        "    '''\n",
        "    dataset_dir = Path(dataset_dir)\n",
        "    rows = []\n",
        "    for class_dir in sorted([p for p in dataset_dir.iterdir() if p.is_dir()]):\n",
        "        label = class_dir.name\n",
        "        for vid in class_dir.rglob(\"*\"):\n",
        "            if vid.is_file() and vid.suffix.lower() in VIDEO_EXTS:\n",
        "                rows.append({\"path\": str(vid), \"label\": label})\n",
        "    if not rows:\n",
        "        raise RuntimeError(f\"No videos found under {dataset_dir}. Expected class subfolders.\")\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    return df\n",
        "\n",
        "def ensure_dir(p: str):\n",
        "    os.makedirs(p, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI9StUhu081K",
        "outputId": "7bd1be6f-fe15-4afa-b917-937b89066ca1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing soccer_rnn/data/indexing.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile soccer_rnn/data/splits.py\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def stratified_split(df: pd.DataFrame, seed: int, test_size: float, val_size: float):\n",
        "    '''\n",
        "    val_size is relative to full set, but we do it as:\n",
        "      train_val, test = split(df, test_size)\n",
        "      train, val = split(train_val, val_size / (1 - test_size))\n",
        "    '''\n",
        "    y = df[\"label\"].values\n",
        "    train_val, test = train_test_split(df, test_size=test_size, random_state=seed, stratify=y)\n",
        "\n",
        "    y_tv = train_val[\"label\"].values\n",
        "    rel_val = val_size / (1.0 - test_size)\n",
        "    train, val = train_test_split(train_val, test_size=rel_val, random_state=seed, stratify=y_tv)\n",
        "\n",
        "    return train.reset_index(drop=True), val.reset_index(drop=True), test.reset_index(drop=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptq_8-9x1TVM",
        "outputId": "a48d1496-3c3e-4929-b068-9f4146060eea"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing soccer_rnn/data/splits.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile soccer_rnn/data/video_decode.py\n",
        "import numpy as np\n",
        "\n",
        "def _uniform_indices(num_frames_total: int, num_samples: int):\n",
        "    if num_frames_total <= 0:\n",
        "        return np.zeros((num_samples,), dtype=np.int64)\n",
        "    if num_frames_total >= num_samples:\n",
        "        return np.linspace(0, num_frames_total - 1, num_samples).round().astype(np.int64)\n",
        "    # pad by repeating last frame\n",
        "    idx = np.linspace(0, num_frames_total - 1, num_frames_total).round().astype(np.int64)\n",
        "    pad = np.full((num_samples - num_frames_total,), idx[-1], dtype=np.int64)\n",
        "    return np.concatenate([idx, pad], axis=0)\n",
        "\n",
        "def decode_frames(path: str, num_frames: int, target_fps: int = None):\n",
        "    '''\n",
        "    Returns uint8 frames: (T, H, W, 3), RGB.\n",
        "    Tries decord first, falls back to cv2.\n",
        "    '''\n",
        "    try:\n",
        "        import decord\n",
        "        from decord import VideoReader, cpu\n",
        "        vr = VideoReader(path, ctx=cpu(0))\n",
        "        total = len(vr)\n",
        "        idx = _uniform_indices(total, num_frames)\n",
        "        frames = vr.get_batch(idx).asnumpy()  # (T,H,W,3) RGB\n",
        "        return frames.astype(np.uint8)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    import cv2\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(f\"Failed to open video: {path}\")\n",
        "\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    idx = _uniform_indices(total, num_frames)\n",
        "    frames = []\n",
        "    wanted = set(idx.tolist())\n",
        "    i = 0\n",
        "    grabbed = {}\n",
        "    while True:\n",
        "        ok, frame = cap.read()\n",
        "        if not ok:\n",
        "            break\n",
        "        if i in wanted:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            grabbed[i] = frame\n",
        "        i += 1\n",
        "    cap.release()\n",
        "\n",
        "    # if total unknown or we missed, do a simpler fallback: reuse last available\n",
        "    if not grabbed:\n",
        "        raise RuntimeError(f\"Could not decode frames from: {path}\")\n",
        "\n",
        "    last = grabbed[max(grabbed.keys())]\n",
        "    for j in idx:\n",
        "        frames.append(grabbed.get(int(j), last))\n",
        "    return np.stack(frames, axis=0).astype(np.uint8)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByG5YRxc1Y3l",
        "outputId": "227f59de-bae7-4c55-c50e-075599d5e2e1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing soccer_rnn/data/video_decode.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile soccer_rnn/data/soccernet_extract.py\n",
        "import json, os, re, subprocess\n",
        "from pathlib import Path\n",
        "from typing import Iterable, List, Dict\n",
        "\n",
        "# SoccerNet label format often includes:\n",
        "#  - \"gameTime\": \"1 - 06:35\" (half - mm:ss)\n",
        "#  - \"position\": \"395728\" (milliseconds from start of match/half)\n",
        "# We prefer \"position\" when present; otherwise parse gameTime.\n",
        "\n",
        "GAME_TIME_RE = re.compile(r\"^\\s*([12])\\s*-\\s*(\\d{1,2}):(\\d{2})\")\n",
        "\n",
        "def _pos_seconds(ann: Dict):\n",
        "    if \"position\" in ann:\n",
        "        try:\n",
        "            return float(ann[\"position\"]) / 1000.0\n",
        "        except Exception:\n",
        "            pass\n",
        "    gt = ann.get(\"gameTime\", \"\")\n",
        "    m = GAME_TIME_RE.match(gt)\n",
        "    if not m:\n",
        "        return None\n",
        "    mm = int(m.group(2))\n",
        "    ss = int(m.group(3))\n",
        "    return float(mm * 60 + ss)\n",
        "\n",
        "def _half_index(ann: Dict):\n",
        "    gt = ann.get(\"gameTime\", \"\")\n",
        "    m = GAME_TIME_RE.match(gt)\n",
        "    if m:\n",
        "        return int(m.group(1))\n",
        "    # fallback: guess 1\n",
        "    return 1\n",
        "\n",
        "def extract_clips_from_labels(\n",
        "    soccernet_game_dir: str,\n",
        "    labels_json_path: str,\n",
        "    out_dir: str,\n",
        "    keep_labels: Iterable[str],\n",
        "    clip_radius_sec: float = 1.5,\n",
        "    video_name_half1: str = \"1_224p.mkv\",\n",
        "    video_name_half2: str = \"2_224p.mkv\",\n",
        "    limit_per_label: int = 400,\n",
        "):\n",
        "    '''\n",
        "    Creates trimmed mp4 clips in:\n",
        "      out_dir/<label>/<game_id>_<half>_<posms>.mp4\n",
        "\n",
        "    Requires ffmpeg (available on Colab).\n",
        "    '''\n",
        "    keep_labels = set(keep_labels)\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with open(labels_json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    anns = data.get(\"annotations\", [])\n",
        "    counts = {k: 0 for k in keep_labels}\n",
        "\n",
        "    game_dir = Path(soccernet_game_dir)\n",
        "    v1 = game_dir / video_name_half1\n",
        "    v2 = game_dir / video_name_half2\n",
        "    if not v1.exists() or not v2.exists():\n",
        "        raise RuntimeError(f\"Missing half videos in {game_dir}. Expected {video_name_half1} and {video_name_half2}\")\n",
        "\n",
        "    for ann in anns:\n",
        "        label = ann.get(\"label\")\n",
        "        if label not in keep_labels:\n",
        "            continue\n",
        "        if counts[label] >= limit_per_label:\n",
        "            continue\n",
        "\n",
        "        half = _half_index(ann)\n",
        "        pos_sec = _pos_seconds(ann)\n",
        "        if pos_sec is None:\n",
        "            continue\n",
        "\n",
        "        start = max(0.0, pos_sec - clip_radius_sec)\n",
        "        dur = 2.0 * clip_radius_sec\n",
        "\n",
        "        label_dir = out_dir / label\n",
        "        label_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        pos_ms = ann.get(\"position\", \"\")\n",
        "        fname = f\"{game_dir.name}_H{half}_P{pos_ms if pos_ms else int(pos_sec*1000)}.mp4\"\n",
        "        out_path = label_dir / fname\n",
        "        if out_path.exists():\n",
        "            counts[label] += 1\n",
        "            continue\n",
        "\n",
        "        src = v1 if half == 1 else v2\n",
        "\n",
        "        cmd = [\n",
        "            \"ffmpeg\", \"-hide_banner\", \"-loglevel\", \"error\",\n",
        "            \"-ss\", str(start), \"-i\", str(src),\n",
        "            \"-t\", str(dur),\n",
        "            \"-vf\", \"scale=320:-2\",\n",
        "            \"-r\", \"25\",\n",
        "            \"-c:v\", \"libx264\", \"-preset\", \"veryfast\", \"-crf\", \"23\",\n",
        "            \"-an\",\n",
        "            str(out_path)\n",
        "        ]\n",
        "        subprocess.run(cmd, check=False)\n",
        "        if out_path.exists() and out_path.stat().st_size > 0:\n",
        "            counts[label] += 1\n",
        "\n",
        "    return counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKOjN9j61XC-",
        "outputId": "754f6562-e8b8-4088-873b-2897bc0d240d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing soccer_rnn/data/soccernet_extract.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile soccer_rnn/models/seq_models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SeqClassifier(nn.Module):\n",
        "    def __init__(self, rnn_type: str, input_dim: int, hidden_size: int, num_layers: int, num_classes: int, dropout: float, bidirectional: bool):\n",
        "        super().__init__()\n",
        "        self.rnn_type = rnn_type.lower()\n",
        "        self.bidirectional = bidirectional\n",
        "        rnn_dropout = dropout if num_layers > 1 else 0.0\n",
        "\n",
        "        if self.rnn_type == \"rnn\":\n",
        "            self.rnn = nn.RNN(\n",
        "                input_size=input_dim,\n",
        "                hidden_size=hidden_size,\n",
        "                num_layers=num_layers,\n",
        "                batch_first=True,\n",
        "                dropout=rnn_dropout,\n",
        "                bidirectional=bidirectional,\n",
        "                nonlinearity=\"tanh\",\n",
        "            )\n",
        "        elif self.rnn_type == \"lstm\":\n",
        "            self.rnn = nn.LSTM(\n",
        "                input_size=input_dim,\n",
        "                hidden_size=hidden_size,\n",
        "                num_layers=num_layers,\n",
        "                batch_first=True,\n",
        "                dropout=rnn_dropout,\n",
        "                bidirectional=bidirectional,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"rnn_type must be one of: rnn, lstm\")\n",
        "\n",
        "        out_dim = hidden_size * (2 if bidirectional else 1)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(out_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, D)\n",
        "        if self.rnn_type == \"lstm\":\n",
        "            out, (h_n, c_n) = self.rnn(x)\n",
        "            # h_n: (num_layers * num_directions, B, H)\n",
        "            if self.bidirectional:\n",
        "                # take last layer forward + backward\n",
        "                h_f = h_n[-2]\n",
        "                h_b = h_n[-1]\n",
        "                h = torch.cat([h_f, h_b], dim=-1)\n",
        "            else:\n",
        "                h = h_n[-1]\n",
        "        else:\n",
        "            out, h_n = self.rnn(x)\n",
        "            if self.bidirectional:\n",
        "                h_f = h_n[-2]\n",
        "                h_b = h_n[-1]\n",
        "                h = torch.cat([h_f, h_b], dim=-1)\n",
        "            else:\n",
        "                h = h_n[-1]\n",
        "\n",
        "        logits = self.head(h)\n",
        "        return logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRdDwXPr1nhx",
        "outputId": "22706bde-66b6-43ab-8dca-79e1d464dfa0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing soccer_rnn/models/seq_models.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile soccer_rnn/models/frame_encoder.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "def build_resnet18_feature_extractor():\n",
        "    m = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "    # remove classifier\n",
        "    backbone = nn.Sequential(*list(m.children())[:-1])  # (B,512,1,1)\n",
        "    backbone.eval()\n",
        "    for p in backbone.parameters():\n",
        "        p.requires_grad = False\n",
        "    return backbone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nript2Vt1ten",
        "outputId": "508e8899-e867-41c2-a6a7-95e02448a33c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing soccer_rnn/models/frame_encoder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile soccer_rnn/data/dataset.py\n",
        "import os, hashlib\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from .video_decode import decode_frames\n",
        "\n",
        "class VideoSeqDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, label_to_idx: dict, feature_extractor, cfg, cache_dir: str):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.label_to_idx = label_to_idx\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.cfg = cfg\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.transform = T.Compose([\n",
        "            T.ToPILImage(),\n",
        "            T.Resize((cfg.image_size, cfg.image_size)),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    def _cache_path(self, video_path: str):\n",
        "        key = f\"{video_path}|T={self.cfg.clip_num_frames}|S={self.cfg.image_size}\"\n",
        "        h = hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n",
        "        return self.cache_dir / f\"{h}.npy\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _compute_features(self, frames_u8: np.ndarray):\n",
        "        # frames_u8: (T,H,W,3) RGB uint8\n",
        "        xs = torch.stack([self.transform(fr) for fr in frames_u8], dim=0)  # (T,3,H,W)\n",
        "        xs = xs.cuda(non_blocking=True)\n",
        "        feats = self.feature_extractor(xs)  # (T,512,1,1)\n",
        "        feats = feats.squeeze(-1).squeeze(-1)  # (T,512)\n",
        "        return feats.float().cpu().numpy()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        row = self.df.iloc[idx]\n",
        "        video_path = row[\"path\"]\n",
        "        label = row[\"label\"]\n",
        "        y = self.label_to_idx[label]\n",
        "\n",
        "        cache_p = self._cache_path(video_path)\n",
        "        if cache_p.exists():\n",
        "            feat = np.load(cache_p)\n",
        "        else:\n",
        "            frames = decode_frames(video_path, num_frames=self.cfg.clip_num_frames, target_fps=self.cfg.clip_fps)\n",
        "            feat = self._compute_features(frames)\n",
        "            np.save(cache_p, feat)\n",
        "\n",
        "        x = torch.from_numpy(feat)  # (T,D)\n",
        "        y = torch.tensor(y, dtype=torch.long)\n",
        "        return x, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXoodFDU1ycH",
        "outputId": "951ae40f-5822-48a4-ac7c-2b0198e380c4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing soccer_rnn/data/dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/soccer_rnn/utils/metrics.py\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "def compute_metrics(y_true, y_pred, labels=None, target_names=None):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
        "    f1w = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "\n",
        "    report = classification_report(\n",
        "        y_true, y_pred,\n",
        "        labels=labels,\n",
        "        target_names=target_names,\n",
        "        digits=4,\n",
        "        zero_division=0\n",
        "    )\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "    # per-class accuracy (recall)\n",
        "    row_sum = cm.sum(axis=1)\n",
        "    diag = np.diag(cm)\n",
        "    per_class_acc = np.divide(diag, row_sum, out=np.zeros_like(diag, dtype=float), where=row_sum != 0)\n",
        "\n",
        "    return {\n",
        "        \"acc\": float(acc),\n",
        "        \"f1_macro\": float(f1m),\n",
        "        \"f1_weighted\": float(f1w),\n",
        "        \"report\": report,\n",
        "        \"cm\": cm,\n",
        "        \"per_class_acc\": per_class_acc,\n",
        "        \"per_class_support\": row_sum,\n",
        "    }\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5sdWzMp2Cqi",
        "outputId": "cff0e4e5-6760-4250-feed-5f7989a35558"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/soccer_rnn/utils/metrics.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile soccer_rnn/utils/plots.py\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_curves(history: dict, out_path: str, title: str):\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    epochs = np.arange(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, history[\"train_loss\"], label=\"train_loss\")\n",
        "    plt.plot(epochs, history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.title(title + \" - loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(out_path.replace(\".png\", \"_loss.png\"), bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, history[\"train_acc\"], label=\"train_acc\")\n",
        "    plt.plot(epochs, history[\"val_acc\"], label=\"val_acc\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.title(title + \" - accuracy\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(out_path.replace(\".png\", \"_acc.png\"), bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names, out_path: str, title: str):\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "\n",
        "    cm = np.asarray(cm)\n",
        "    n = len(class_names)\n",
        "\n",
        "    plt.figure(figsize=(max(6, n * 1.6), max(5, n * 1.3)))\n",
        "    plt.imshow(cm, interpolation=\"nearest\")\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    ticks = np.arange(n)\n",
        "    plt.xticks(ticks, class_names, rotation=45, ha=\"right\")\n",
        "    plt.yticks(ticks, class_names)\n",
        "    plt.ylabel(\"true\")\n",
        "    plt.xlabel(\"pred\")\n",
        "\n",
        "    maxv = cm.max() if cm.size else 0\n",
        "    thresh = maxv * 0.5\n",
        "\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            val = int(cm[i, j])\n",
        "            color = \"white\" if cm[i, j] > thresh else \"black\"\n",
        "            plt.text(j, i, str(val), ha=\"center\", va=\"center\", color=color, fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "\n",
        "def bar_compare(names, values, out_path: str, title: str, ylabel: str):\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.bar(names, values)\n",
        "    plt.title(title)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.xticks(rotation=20, ha=\"right\")\n",
        "    plt.grid(True, axis=\"y\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_split_distribution(train_counts, val_counts, test_counts, class_names, out_path: str, title: str):\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "\n",
        "    train_counts = np.array(train_counts, dtype=int)\n",
        "    val_counts = np.array(val_counts, dtype=int)\n",
        "    test_counts = np.array(test_counts, dtype=int)\n",
        "\n",
        "    x = np.arange(len(class_names))\n",
        "    width = 0.25\n",
        "\n",
        "    plt.figure(figsize=(max(7, len(class_names) * 1.6), 4.8))\n",
        "    plt.bar(x - width, train_counts, width, label=\"train\")\n",
        "    plt.bar(x,         val_counts,   width, label=\"val\")\n",
        "    plt.bar(x + width, test_counts,  width, label=\"test\")\n",
        "\n",
        "    plt.xticks(x, class_names, rotation=35, ha=\"right\")\n",
        "    plt.ylabel(\"count\")\n",
        "    plt.title(title)\n",
        "    plt.grid(True, axis=\"y\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, bbox_inches=\"tight\")\n",
        "    plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Jk2jlqT2DAd",
        "outputId": "e8559cef-d9cb-4dd4-d46b-c459307d16f5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing soccer_rnn/utils/plots.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/soccer_rnn/train_eval.py\n",
        "import os, time\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "def run_one_epoch(model, loader, optimizer, scaler, device, train: bool):\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    losses = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    use_amp = (scaler is not None) and (device.type == \"cuda\")\n",
        "\n",
        "    # IMPORTANT: no tqdm here (keeps output clean)\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n",
        "            logits = model(x)\n",
        "            loss = ce(logits, y)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            if use_amp:\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "        losses.append(loss.detach().float().cpu().item())\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.numel()\n",
        "\n",
        "    return float(np.mean(losses)), float(correct / max(1, total))\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict(model, loader, device):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    for x, y in loader:  # no tqdm to keep logs clean\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        logits = model(x)\n",
        "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
        "        ps.extend(pred)\n",
        "        ys.extend(y.numpy().tolist())\n",
        "    return np.array(ys), np.array(ps)\n",
        "\n",
        "def fit_model(model, train_loader, val_loader, cfg, run_dir: str, name: str):\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    use_amp = bool(cfg.use_amp and device.type == \"cuda\")\n",
        "    scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)\n",
        "\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "    best_val_acc = -1.0\n",
        "    best_path = None\n",
        "\n",
        "    pbar = tqdm(range(1, cfg.epochs + 1), desc=f\"{name}\", unit=\"epoch\", leave=True)\n",
        "    for epoch in pbar:\n",
        "        t0 = time.time()\n",
        "        tr_loss, tr_acc = run_one_epoch(model, train_loader, optimizer, scaler if use_amp else None, device, train=True)\n",
        "        va_loss, va_acc = run_one_epoch(model, val_loader, optimizer, scaler if use_amp else None, device, train=False)\n",
        "        dt = time.time() - t0\n",
        "\n",
        "        history[\"train_loss\"].append(tr_loss)\n",
        "        history[\"val_loss\"].append(va_loss)\n",
        "        history[\"train_acc\"].append(tr_acc)\n",
        "        history[\"val_acc\"].append(va_acc)\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            \"tr_loss\": f\"{tr_loss:.4f}\",\n",
        "            \"va_loss\": f\"{va_loss:.4f}\",\n",
        "            \"tr_acc\": f\"{tr_acc:.3f}\",\n",
        "            \"va_acc\": f\"{va_acc:.3f}\",\n",
        "            \"sec\": f\"{dt:.1f}\",\n",
        "        })\n",
        "\n",
        "        ckpt = {\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"history\": history,\n",
        "            \"cfg\": cfg.__dict__,\n",
        "        }\n",
        "\n",
        "        if va_acc > best_val_acc:\n",
        "            best_val_acc = va_acc\n",
        "            best_path = os.path.join(run_dir, f\"{name}_best.pt\")\n",
        "            torch.save(ckpt, best_path)\n",
        "\n",
        "    return history, best_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1D5lw8u2G2W",
        "outputId": "f5633e24-d68d-429f-c133-477f3947d27c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/soccer_rnn/train_eval.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_colab.py\n",
        "import os, json, shutil\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from soccer_rnn.config import Config\n",
        "from soccer_rnn.utils.seed import set_seed\n",
        "from soccer_rnn.data.indexing import build_index_folder_per_class, ensure_dir\n",
        "from soccer_rnn.data.splits import stratified_split\n",
        "from soccer_rnn.models.frame_encoder import build_resnet18_feature_extractor\n",
        "from soccer_rnn.data.dataset import VideoSeqDataset\n",
        "from soccer_rnn.models.seq_models import SeqClassifier\n",
        "from soccer_rnn.train_eval import fit_model, predict\n",
        "from soccer_rnn.utils.metrics import compute_metrics\n",
        "from soccer_rnn.utils.plots import plot_curves, plot_confusion_matrix, bar_compare, plot_split_distribution\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def main():\n",
        "    cfg = Config()\n",
        "\n",
        "    # edit these in the Colab cell that calls this script\n",
        "    dataset_source = os.environ.get(\"DATASET_SOURCE\", cfg.dataset_source)\n",
        "    cfg.dataset_source = dataset_source\n",
        "\n",
        "    RAW_ROOT = os.environ.get(\"RAW_ROOT\", cfg.raw_root)\n",
        "    PROC_ROOT = os.environ.get(\"PROC_ROOT\", cfg.processed_root)\n",
        "    cfg.raw_root = RAW_ROOT\n",
        "    cfg.processed_root = PROC_ROOT\n",
        "\n",
        "    set_seed(cfg.seed)\n",
        "    ensure_dir(cfg.out_root)\n",
        "    ensure_dir(cfg.cache_root)\n",
        "\n",
        "    # 1) build index.csv (path,label)\n",
        "    index_csv = os.path.join(cfg.processed_root, \"index.csv\")\n",
        "    ensure_dir(cfg.processed_root)\n",
        "\n",
        "    if not os.path.exists(index_csv):\n",
        "        # user must ensure clips exist in RAW_ROOT/clips/<class>/*.mp4\n",
        "        clips_dir = os.path.join(cfg.raw_root, \"clips\")\n",
        "        df = build_index_folder_per_class(clips_dir, index_csv)\n",
        "    else:\n",
        "        df = pd.read_csv(index_csv)\n",
        "\n",
        "    # 2) split\n",
        "    train_df, val_df, test_df = stratified_split(df, cfg.seed, cfg.test_size, cfg.val_size)\n",
        "    train_df.to_csv(os.path.join(cfg.processed_root, \"train.csv\"), index=False)\n",
        "    val_df.to_csv(os.path.join(cfg.processed_root, \"val.csv\"), index=False)\n",
        "    test_df.to_csv(os.path.join(cfg.processed_root, \"test.csv\"), index=False)\n",
        "\n",
        "    classes = sorted(df[\"label\"].unique().tolist())\n",
        "    label_to_idx = {c: i for i, c in enumerate(classes)}\n",
        "    idx_to_label = {i: c for c, i in label_to_idx.items()}\n",
        "    print(\"classes:\", classes)\n",
        "\n",
        "    # class distribution plot (train/val/test) as grouped bars\n",
        "    train_counts = [int((train_df[\"label\"] == c).sum()) for c in classes]\n",
        "    val_counts   = [int((val_df[\"label\"] == c).sum()) for c in classes]\n",
        "    test_counts  = [int((test_df[\"label\"] == c).sum()) for c in classes]\n",
        "\n",
        "    plot_split_distribution(\n",
        "        train_counts, val_counts, test_counts,\n",
        "        classes,\n",
        "        out_path=os.path.join(cfg.out_root, \"soccer_seq\", \"split_distribution.png\"),\n",
        "        title=\"Class distribution across splits\",\n",
        "    )\n",
        "    print(\"Saved split distribution to:\", os.path.join(cfg.out_root, \"soccer_seq\", \"split_distribution.png\"))\n",
        "\n",
        "    # 3) feature extractor\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    feat_extractor = build_resnet18_feature_extractor().to(device)\n",
        "\n",
        "    # 4) datasets/loaders\n",
        "    train_ds = VideoSeqDataset(train_df, label_to_idx, feat_extractor, cfg, cache_dir=os.path.join(cfg.cache_root, \"train\"))\n",
        "    val_ds   = VideoSeqDataset(val_df,   label_to_idx, feat_extractor, cfg, cache_dir=os.path.join(cfg.cache_root, \"val\"))\n",
        "    test_ds  = VideoSeqDataset(test_df,  label_to_idx, feat_extractor, cfg, cache_dir=os.path.join(cfg.cache_root, \"test\"))\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,  num_workers=cfg.num_workers, pin_memory=True)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
        "    test_loader  = DataLoader(test_ds,  batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
        "\n",
        "    # 5) train models\n",
        "    run_root = os.path.join(cfg.out_root, \"soccer_seq\")\n",
        "    ensure_dir(run_root)\n",
        "\n",
        "    results = {}\n",
        "    model_specs = [\n",
        "        (\"rnn\",   dict(rnn_type=\"rnn\",  bidirectional=False)),\n",
        "        (\"lstm\",  dict(rnn_type=\"lstm\", bidirectional=False)),\n",
        "        (\"bilstm\",dict(rnn_type=\"lstm\", bidirectional=True)),\n",
        "    ]\n",
        "\n",
        "    for name, spec in model_specs:\n",
        "        model = SeqClassifier(\n",
        "            rnn_type=spec[\"rnn_type\"],\n",
        "            input_dim=cfg.feature_dim,\n",
        "            hidden_size=cfg.hidden_size,\n",
        "            num_layers=cfg.num_layers,\n",
        "            num_classes=len(classes),\n",
        "            dropout=cfg.dropout,\n",
        "            bidirectional=spec[\"bidirectional\"],\n",
        "        )\n",
        "        run_dir = os.path.join(run_root, name)\n",
        "        history, best_path = fit_model(model, train_loader, val_loader, cfg, run_dir, name)\n",
        "        plot_curves(history, os.path.join(run_dir, f\"{name}.png\"), title=name)\n",
        "\n",
        "        # load best\n",
        "        ckpt = torch.load(best_path, map_location=\"cpu\")\n",
        "        model.load_state_dict(ckpt[\"model_state\"])\n",
        "        model = model.to(device)\n",
        "\n",
        "        y_true, y_pred = predict(model, test_loader, device)\n",
        "        metrics = compute_metrics(y_true, y_pred, labels=list(range(len(classes))), target_names=classes)\n",
        "        print(\"\\n\", name, \"test acc\", metrics[\"acc\"], \"macro_f1\", metrics[\"f1_macro\"])\n",
        "        print(metrics[\"report\"])\n",
        "\n",
        "        plot_confusion_matrix(metrics[\"cm\"], classes, os.path.join(run_dir, f\"{name}_cm.png\"), title=f\"{name} confusion matrix\")\n",
        "\n",
        "        # per-class accuracy\n",
        "        pca = metrics[\"per_class_acc\"]\n",
        "        print(\"per-class accuracy:\")\n",
        "        for cls, v in zip(classes, pca):\n",
        "            print(f\"  {cls}: {v:.4f}\")\n",
        "\n",
        "        # plot per-class accuracy as a bar chart (uses bar_compare from plots.py) :contentReference[oaicite:0]{index=0}\n",
        "        bar_compare(\n",
        "            classes,\n",
        "            pca.tolist(),\n",
        "            os.path.join(run_dir, f\"{name}_per_class_acc.png\"),\n",
        "            title=f\"{name} per-class accuracy\",\n",
        "            ylabel=\"accuracy\",\n",
        "        )\n",
        "\n",
        "        results[name] = {\n",
        "            \"acc\": metrics[\"acc\"],\n",
        "            \"f1_macro\": metrics[\"f1_macro\"],\n",
        "            \"f1_weighted\": metrics[\"f1_weighted\"],\n",
        "            \"best_ckpt\": best_path,\n",
        "        }\n",
        "\n",
        "    # 6) comparison plots\n",
        "    names = list(results.keys())\n",
        "    bar_compare(names, [results[n][\"acc\"] for n in names], os.path.join(run_root, \"compare_acc.png\"), \"model comparison\", \"test accuracy\")\n",
        "    bar_compare(names, [results[n][\"f1_macro\"] for n in names], os.path.join(run_root, \"compare_f1macro.png\"), \"model comparison\", \"macro F1\")\n",
        "\n",
        "    # save results.json\n",
        "    with open(os.path.join(run_root, \"results.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    print(\"\\nSaved everything to:\", run_root)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3cc4a7czjuO",
        "outputId": "8047c437-add1-4a76-aaba-d0e908b8bca3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting run_colab.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## s1"
      ],
      "metadata": {
        "id": "ahWMoQtvEbYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kaggle download (trimmed clips dataset)"
      ],
      "metadata": {
        "id": "W5mSi1POvrzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/data_raw\n",
        "!pip -q install kaggle\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle config view\n",
        "\n",
        "# download + unzip (overwrite without prompting)\n",
        "!kaggle datasets download -d itarek898/football-match-actions-video-dataset -p /content/data_raw\n",
        "!unzip -q -o /content/data_raw/football-match-actions-video-dataset.zip -d /content/data_raw/kaggle_ds\n",
        "\n",
        "# recreate clips so nothing prompts later\n",
        "!rm -rf /content/data_raw/clips\n",
        "!mkdir -p /content/data_raw/clips\n",
        "\n",
        "# symlink the class folders (spaces handled with quotes)\n",
        "!ln -s \"/content/data_raw/kaggle_ds/football match action video dataset/\"* /content/data_raw/clips/\n",
        "\n",
        "# verify\n",
        "!ls -lah /content/data_raw/clips | head -n 30\n",
        "!find /content/data_raw/clips -maxdepth 2 -type f | head -n 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l64aKFGh2WQg",
        "outputId": "68fd1dbf-f98a-452f-8417-1b754cbbd526"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration values from /root/.kaggle\n",
            "- username: omidnaeej\n",
            "- path: None\n",
            "- proxy: None\n",
            "- competition: None\n",
            "Dataset URL: https://www.kaggle.com/datasets/itarek898/football-match-actions-video-dataset\n",
            "License(s): MIT\n",
            "Downloading football-match-actions-video-dataset.zip to /content/data_raw\n",
            "100% 2.77G/2.78G [00:27<00:00, 83.3MB/s]\n",
            "100% 2.78G/2.78G [00:27<00:00, 108MB/s] \n",
            "total 20K\n",
            "drwxr-xr-x 2 root root 4.0K Dec 29 17:28 .\n",
            "drwxr-xr-x 4 root root 4.0K Dec 29 17:28 ..\n",
            "lrwxrwxrwx 1 root root   72 Dec 29 17:28 Red Card -> /content/data_raw/kaggle_ds/football match action video dataset/Red Card\n",
            "lrwxrwxrwx 1 root root   71 Dec 29 17:28 scoring -> /content/data_raw/kaggle_ds/football match action video dataset/scoring\n",
            "lrwxrwxrwx 1 root root   71 Dec 29 17:28 takling -> /content/data_raw/kaggle_ds/football match action video dataset/takling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "5-lgO6JWEeYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!export DATASET_SOURCE=\"kaggle\"   # or \"soccernet\" (doesn’t change training; only for your own tracking)\n",
        "!export RAW_ROOT=\"/content/data_raw\"\n",
        "!export PROC_ROOT=\"/content/data_processed\"\n",
        "!python /content/run_colab.py\n"
      ],
      "metadata": {
        "id": "FCe7BDJA2i9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b93d1aa-946b-4557-c058-1c7d85b02793"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classes: ['Red Card', 'scoring', 'takling']\n",
            "Saved split distribution to: /content/runs/soccer_seq/split_distribution.png\n",
            "rnn: 100% 10/10 [00:02<00:00,  4.86epoch/s, tr_loss=0.0093, va_loss=0.1958, tr_acc=1.000, va_acc=0.951, sec=0.2]\n",
            "\n",
            " rnn test acc 0.8524590163934426 macro_f1 0.7670397257511595\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Red Card     0.8571    0.6667    0.7500         9\n",
            "     scoring     0.8696    0.9756    0.9195        41\n",
            "     takling     0.7500    0.5455    0.6316        11\n",
            "\n",
            "    accuracy                         0.8525        61\n",
            "   macro avg     0.8256    0.7292    0.7670        61\n",
            "weighted avg     0.8462    0.8525    0.8426        61\n",
            "\n",
            "per-class accuracy:\n",
            "  Red Card: 0.6667\n",
            "  scoring: 0.9756\n",
            "  takling: 0.5455\n",
            "lstm: 100% 10/10 [00:02<00:00,  4.81epoch/s, tr_loss=0.0775, va_loss=0.2199, tr_acc=0.968, va_acc=0.934, sec=0.2]\n",
            "\n",
            " lstm test acc 0.9344262295081968 macro_f1 0.8989495798319328\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Red Card     1.0000    0.7778    0.8750         9\n",
            "     scoring     0.9318    1.0000    0.9647        41\n",
            "     takling     0.9000    0.8182    0.8571        11\n",
            "\n",
            "    accuracy                         0.9344        61\n",
            "   macro avg     0.9439    0.8653    0.8989        61\n",
            "weighted avg     0.9361    0.9344    0.9321        61\n",
            "\n",
            "per-class accuracy:\n",
            "  Red Card: 0.7778\n",
            "  scoring: 1.0000\n",
            "  takling: 0.8182\n",
            "bilstm: 100% 10/10 [00:02<00:00,  3.36epoch/s, tr_loss=0.0231, va_loss=0.2793, tr_acc=0.996, va_acc=0.918, sec=0.3]\n",
            "\n",
            " bilstm test acc 0.8360655737704918 macro_f1 0.7522739818081455\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Red Card     0.8750    0.7778    0.8235         9\n",
            "     scoring     0.8667    0.9512    0.9070        41\n",
            "     takling     0.6250    0.4545    0.5263        11\n",
            "\n",
            "    accuracy                         0.8361        61\n",
            "   macro avg     0.7889    0.7278    0.7523        61\n",
            "weighted avg     0.8243    0.8361    0.8260        61\n",
            "\n",
            "per-class accuracy:\n",
            "  Red Card: 0.7778\n",
            "  scoring: 0.9512\n",
            "  takling: 0.4545\n",
            "\n",
            "Saved everything to: /content/runs/soccer_seq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save to Drive\n"
      ],
      "metadata": {
        "id": "085yseJJEg4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rsync -av \\\n",
        "  --exclude='/.config' \\\n",
        "  --exclude='/drive' \\\n",
        "  --exclude='/data_raw' \\\n",
        "  --exclude='/data_processed' \\\n",
        "  --exclude='/sample_data' \\\n",
        "  --exclude='/feature_cache' \\\n",
        "  /content/ /content/drive/MyDrive/GAR/Soccer-kaggle/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Da1mMUbKzVQc",
        "outputId": "0b33bb67-001c-411a-aadb-a36840cc8baf"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sending incremental file list\n",
            "./\n",
            "kaggle.json\n",
            "run_colab.py\n",
            "runs/\n",
            "runs/soccer_seq/\n",
            "runs/soccer_seq/compare_acc.png\n",
            "runs/soccer_seq/compare_f1macro.png\n",
            "runs/soccer_seq/results.json\n",
            "runs/soccer_seq/split_distribution.png\n",
            "runs/soccer_seq/bilstm/\n",
            "runs/soccer_seq/bilstm/bilstm_acc.png\n",
            "runs/soccer_seq/bilstm/bilstm_best.pt\n",
            "runs/soccer_seq/bilstm/bilstm_cm.png\n",
            "runs/soccer_seq/bilstm/bilstm_loss.png\n",
            "runs/soccer_seq/bilstm/bilstm_per_class_acc.png\n",
            "runs/soccer_seq/lstm/\n",
            "runs/soccer_seq/lstm/lstm_acc.png\n",
            "runs/soccer_seq/lstm/lstm_best.pt\n",
            "runs/soccer_seq/lstm/lstm_cm.png\n",
            "runs/soccer_seq/lstm/lstm_loss.png\n",
            "runs/soccer_seq/lstm/lstm_per_class_acc.png\n",
            "runs/soccer_seq/rnn/\n",
            "runs/soccer_seq/rnn/rnn_acc.png\n",
            "runs/soccer_seq/rnn/rnn_best.pt\n",
            "runs/soccer_seq/rnn/rnn_cm.png\n",
            "runs/soccer_seq/rnn/rnn_loss.png\n",
            "runs/soccer_seq/rnn/rnn_per_class_acc.png\n",
            "soccer_rnn/\n",
            "soccer_rnn/__init__.py\n",
            "soccer_rnn/config.py\n",
            "soccer_rnn/train_eval.py\n",
            "soccer_rnn/.ipynb_checkpoints/\n",
            "soccer_rnn/__pycache__/\n",
            "soccer_rnn/__pycache__/__init__.cpython-312.pyc\n",
            "soccer_rnn/__pycache__/config.cpython-312.pyc\n",
            "soccer_rnn/__pycache__/train_eval.cpython-312.pyc\n",
            "soccer_rnn/data/\n",
            "soccer_rnn/data/dataset.py\n",
            "soccer_rnn/data/indexing.py\n",
            "soccer_rnn/data/soccernet_extract.py\n",
            "soccer_rnn/data/splits.py\n",
            "soccer_rnn/data/video_decode.py\n",
            "soccer_rnn/data/__pycache__/\n",
            "soccer_rnn/data/__pycache__/dataset.cpython-312.pyc\n",
            "soccer_rnn/data/__pycache__/indexing.cpython-312.pyc\n",
            "soccer_rnn/data/__pycache__/splits.cpython-312.pyc\n",
            "soccer_rnn/data/__pycache__/video_decode.cpython-312.pyc\n",
            "soccer_rnn/models/\n",
            "soccer_rnn/models/frame_encoder.py\n",
            "soccer_rnn/models/seq_models.py\n",
            "soccer_rnn/models/__pycache__/\n",
            "soccer_rnn/models/__pycache__/frame_encoder.cpython-312.pyc\n",
            "soccer_rnn/models/__pycache__/seq_models.cpython-312.pyc\n",
            "soccer_rnn/utils/\n",
            "soccer_rnn/utils/metrics.py\n",
            "soccer_rnn/utils/plots.py\n",
            "soccer_rnn/utils/seed.py\n",
            "soccer_rnn/utils/__pycache__/\n",
            "soccer_rnn/utils/__pycache__/metrics.cpython-312.pyc\n",
            "soccer_rnn/utils/__pycache__/plots.cpython-312.pyc\n",
            "soccer_rnn/utils/__pycache__/seed.cpython-312.pyc\n",
            "\n",
            "sent 31,242,908 bytes  received 977 bytes  6,943,085.56 bytes/sec\n",
            "total size is 31,231,309  speedup is 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SoccerNet"
      ],
      "metadata": {
        "id": "3ukV54AYv080"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "pw = getpass(\"SoccerNet video password (from NDA): \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wjg9wRvU48zV",
        "outputId": "dd5f76c4-72a8-4024-fced-9b2bffb9e31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SoccerNet video password (from NDA): ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "from soccer_rnn.data.soccernet_extract import extract_clips_from_labels\n",
        "from soccer_rnn.data.indexing import ensure_dir\n",
        "\n",
        "RAW_ROOT = \"/content/data_raw\"\n",
        "ensure_dir(RAW_ROOT)\n",
        "\n",
        "# 1) download SoccerNet via official pip package (requires password for videos)\n",
        "!pip -q install SoccerNet\n",
        "\n",
        "from SoccerNet.Downloader import SoccerNetDownloader as SNdl\n",
        "dl = SNdl(LocalDirectory=f\"{RAW_ROOT}/SoccerNet\")\n",
        "# pw = input(\"SoccerNet video password (from NDA): \").strip()\n",
        "\n",
        "# ball spotting task download (as referenced in SoccerNet devkit)\n",
        "dl.downloadDataTask(task=\"spotting-ball-2024\", split=[\"train\",\"valid\",\"test\"], password=pw)\n",
        "\n",
        "# 2) extract short clips around timestamps to make a normal classification dataset\n",
        "# expected per-game folders under RAW_ROOT/SoccerNet/... (depends on SoccerNet downloader layout)\n",
        "# You may need to set GAME_DIRS manually if your folder structure differs.\n",
        "KEEP = [\"Pass\", \"Drive\", \"Shot\"]  # Drive is often used as ball-carry/dribble-like\n",
        "OUT_CLIPS = f\"{RAW_ROOT}/clips\"\n",
        "ensure_dir(OUT_CLIPS)\n",
        "\n",
        "# Example scan: find labels-ball.json and its parent game directory\n",
        "import pathlib\n",
        "labels = list(pathlib.Path(f\"{RAW_ROOT}/SoccerNet\").rglob(\"Labels-ball.json\"))\n",
        "print(\"found Labels-ball.json files:\", len(labels))\n",
        "\n",
        "total = {}\n",
        "for lab_path in labels[:10]:  # keep it small first\n",
        "    game_dir = str(lab_path.parent)\n",
        "    counts = extract_clips_from_labels(\n",
        "        soccernet_game_dir=game_dir,\n",
        "        labels_json_path=str(lab_path),\n",
        "        out_dir=OUT_CLIPS,\n",
        "        keep_labels=KEEP,\n",
        "        clip_radius_sec=1.5,\n",
        "        video_name_half1=\"1_224p.mkv\",\n",
        "        video_name_half2=\"2_224p.mkv\",\n",
        "        limit_per_label=300,\n",
        "    )\n",
        "    for k,v in counts.items():\n",
        "        total[k] = total.get(k, 0) + v\n",
        "\n",
        "print(\"extracted:\", total)\n",
        "print(\"clips dir:\", OUT_CLIPS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "ND_lyNRMwCe9",
        "outputId": "59b9cc82-c14f-4a39-f87b-0f9f60c5300b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading /content/data_raw/SoccerNet/spotting-ball-2024/train.zip...: : 8.45GiB [08:25, 16.7MiB/s]                         \n",
            "Downloading /content/data_raw/SoccerNet/spotting-ball-2024/valid.zip...:  27%|██▋       | 550M/2.04G [31:16<1:25:53, 290kiB/s]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-772674334.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# ball spotting task download (as referenced in SoccerNet devkit)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloadDataTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"spotting-ball-2024\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# 2) extract short clips around timestamps to make a normal classification dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/SoccerNet/Downloader.py\u001b[0m in \u001b[0;36mdownloadDataTask\u001b[0;34m(self, task, split, verbose, password, version, source)\u001b[0m\n\u001b[1;32m    561\u001b[0m                                         verbose=verbose)\n\u001b[1;32m    562\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"valid\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m                 res = self.downloadFile(path_local=os.path.join(self.LocalDirectory, task, \"valid.zip\"),\n\u001b[0m\u001b[1;32m    564\u001b[0m                                         path_owncloud=os.path.join(self.OwnCloudServer, \"valid.zip\").replace(\n\u001b[1;32m    565\u001b[0m                                             ' ', '%20').replace('\\\\', '/'),\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/SoccerNet/Downloader.py\u001b[0m in \u001b[0;36mdownloadFile\u001b[0;34m(self, path_local, path_owncloud, user, password, verbose)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_local\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 urllib.request.urlretrieve(\n\u001b[0m\u001b[1;32m     73\u001b[0m                     path_owncloud, path_local, MyProgressBar(path_local))\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocknum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m                 \u001b[0mread\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;31m# clip the read to the \"end of response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m                 \u001b[0mamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1249\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export DATASET_SOURCE=\"soccernet\"\n",
        "!export RAW_ROOT=\"/content/data_raw\"\n",
        "!export PROC_ROOT=\"/content/data_processed\"\n",
        "!python /content/soccer_rnn_project/run_colab.py\n"
      ],
      "metadata": {
        "id": "k7OZQJPD1Op4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rsync -av \\\n",
        "  --exclude='/.config' \\\n",
        "  --exclude='/drive' \\\n",
        "  --exclude='/data_raw' \\\n",
        "  --exclude='/data_processed' \\\n",
        "  --exclude='/sample_data' \\\n",
        "  --exclude='/feature_cache' \\\n",
        "  /content/ /content/drive/MyDrive/GAR/Soccer-soccernet/"
      ],
      "metadata": {
        "id": "OzrbV6Tu1Vhm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}